---
title: "PML Prediction Report"
author: "Zhao JianZhuang"
---

##Summary

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. In this project, the main goal is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to predict how well they were doing the exercise using a relatively simple prediction model. The raw data can be obtained from <http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises>.

##Processing Data

Read the data into R.

```{r}
library (lattice)
library(ggplot2)
library(knitr)
library(caret)
library(corrplot)
library(randomForest)
data_training <- read.csv("pml-training.csv", na.strings= c("NA",""," "))
data_testing <- read.csv("pml-testing.csv", na.strings= c("NA",""," "))
```

There are loads of NA values in the data. We need to clean and remove these columns from the data set. The first eight columns that acted as identifiers for the experiment were also removed.

```{r}
training_NAs <- apply(data_training, 2, function(x) {sum(is.na(x))})
training_new <- data_training[,which(training_NAs == 0)]
testing_NAs <- apply(data_testing, 2, function(x) {sum(is.na(x))})
testing_new <- data_testing[,which(testing_NAs == 0)]
training_final <- training_new[8:length(training_new)]
testing_final  <- testing_new[8:length(testing_new)]
```

##Training a Model

The test data set was split up into training and cross validation sets in a 70:30 ratio in order to train the model and then test it against data it was not specifically fitted to.

```{r}
inTrain <-  createDataPartition(y = training_final$classe, p = 0.7, list = FALSE)
training <- training_final[inTrain, ]
crossval <- training_final[-inTrain, ]
```

A random forest model was selected to predict the classification. The correllation plot was used to see how strong the variables relationships are with each other.

```{r}
corMatrix <- cor(training[, -length(training)])
corrplot(corMatrix, order = "FPC", method = "circle", type = "lower", tl.cex = 0.8,  tl.col = rgb(0, 0, 0))
```

In the graph, the dark red and blue colours indicate a highly negative and positive relationship respectively between the variables. There was not much concern for highly correlated predictors which mean that all of them can be contained in the model.

And then, a model was fitted with the outcome set to the training class and all the other variables used to predict. 

```{r}
model <- randomForest(classe ~ ., data = training)
model
```

The model produced a very small OOB estimate of error rate of 0.5%. This was deemed good enough to progress the testing.

##Model Cross Variation

The model was further used to classify the remaining 30% of cross validation data. 
```{r}
predictCross <- predict(model, crossval)
confusionMatrix(crossval$classe, predictCross)
```

The confusion matrix and statistics shows that this model has a 99.51% prediction accuracy. Again, this model was proved good enough to predict new data.

##Prediction of Testing Data

A separate data set was then loaded into R and cleaned in the same manner as before. The model was then used to predict the classifications of the 20 results of this new data.

```{r}
predTesting <- predict(model, testing_final)
predTesting
```

##Conclusion

With the abundance of information given from multiple measuring instruments it's possible to accurately predict how well a person is preforming an excercise using a relatively simple prediction model.



